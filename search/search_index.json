{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Learning-to-Rank in PyTorch \u00b6 Introduction \u00b6 This open-source project, referred to as PTRanking (Learning-to-Rank in PyTorch) aims to provide scalable and extendable implementations of typical learning-to-rank methods based on PyTorch. On one hand, this project enables a uniform comparison over several benchmark datasets, leading to an in-depth understanding of previous learning-to-rank methods. On the other hand, this project makes it easy to develop and incorporate newly proposed models, so as to expand the territory of techniques on learning-to-rank. Key Features : A number of representative learning-to-rank models for addressing Ad-hoc Ranking and Search Result Diversification , including not only the traditional optimization framework via empirical risk minimization but also the adversarial optimization framework Supports widely used benchmark datasets. Meanwhile, random masking of the ground-truth labels with a specified ratio is also supported Supports different metrics, such as Precision, MAP, nDCG, nERR, alpha-nDCG and ERR-IA. Highly configurable functionalities for fine-tuning hyper-parameters, e.g., grid-search over hyper-parameters of a specific model Provides easy-to-use APIs for developing a new learning-to-rank model Source Code \u00b6 Please refer to the Github Repository PT-Ranking for detailed implementations. Implemented models \u00b6 Typical Learning-to-Rank Methods for Ad-hoc Ranking Model Pointwise RankMSE Pairwise RankNet Listwise ListNet \u30fb ListMLE \u30fb RankCosine \u30fb LambdaRank \u30fb ApproxNDCG \u30fb WassRank \u30fb STListNet \u30fb LambdaLoss Learning-to-Rank Methods for Search Result Diversification Model Score-and-sort strategy MO4SRD \u30fb DALETOR Adversarial Learning-to-Rank Methods for Ad-hoc Ranking Model Pointwise IR_GAN_Point Pairwise IR_GAN_Pair Listwise IR_GAN_List Learning-to-rank Methods Based on Gradient Boosting Decision Trees (GBDT) (based on LightGBM) Model Listwise LightGBMLambdaMART References \u00b6 RankNet : Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd ICML. 89\u201396. RankSVM : Joachims, Thorsten. Optimizing Search Engines Using Clickthrough Data. Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 133\u2013142, 2002. LambdaRank : Christopher J.C. Burges, Robert Ragno, and Quoc Viet Le. 2006. Learning to Rank with Nonsmooth Cost Functions. In Proceedings of NIPS conference. 193\u2013200. ListNet : Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to Rank: From Pairwise Approach to Listwise Approach. In Proceedings of the 24th ICML. 129\u2013136. ListMLE : Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwise Approach to Learning to Rank: Theory and Algorithm. In Proceedings of the 25th ICML. 1192\u20131199. RankCosine : Tao Qin, Xu-Dong Zhang, Ming-Feng Tsai, De-Sheng Wang, Tie-Yan Liu, and Hang Li. 2008. Query-level loss functions for information retrieval. Information Processing and Management 44, 2 (2008), 838\u2013855. AppoxNDCG : Tao Qin, Tie-Yan Liu, and Hang Li. 2010. A general approximation framework for direct optimization of information retrieval measures. Journal of Information Retrieval 13, 4 (2010), 375\u2013397. LambdaMART : Q. Wu, C.J.C. Burges, K. Svore and J. Gao. Adapting Boosting for Information Retrieval Measures. Journal of Information Retrieval, 2007. (We note that the implementation is provided by LightGBM ) IRGAN : Wang, Jun and Yu, Lantao and Zhang, Weinan and Gong, Yu and Xu, Yinghui and Wang, Benyou and Zhang, Peng and Zhang, Dell. IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models. Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, 515\u2013524, 2017. ( Besides the pointwise and pairiwse adversarial learning-to-rank methods introduced in the paper, we also include the listwise version in PT-Ranking ) LambdaLoss Xuanhui Wang, Cheng Li, Nadav Golbandi, Mike Bendersky and Marc Najork. The LambdaLoss Framework for Ranking Metric Optimization. Proceedings of The 27th ACM International Conference on Information and Knowledge Management (CIKM '18), 1313-1322, 2018. WassRank : Hai-Tao Yu, Adam Jatowt, Hideo Joho, Joemon Jose, Xiao Yang and Long Chen. WassRank: Listwise Document Ranking Using Optimal Transport Theory. Proceedings of the 12th International Conference on Web Search and Data Mining (WSDM), 24-32, 2019. Bruch, Sebastian and Han, Shuguang and Bendersky, Michael and Najork, Marc. A Stochastic Treatment of Learning to Rank Scoring Functions. Proceedings of the 13th International Conference on Web Search and Data Mining (WSDM), 61\u201369, 2020. MO4SRD : Hai-Tao Yu. Optimize What You EvaluateWith: Search Result Diversification Based on Metric Optimization. The 36th AAAI Conference on Artificial Intelligence, 2022. DALETOR : Le Yan, Zhen Qin, Rama Kumar Pasumarthi, Xuanhui Wang, Michael Bendersky. Diversification-Aware Learning to Rank using Distributed Representation. In Proceedings of the Web Conference 2021, 127\u2013136. Test Setting \u00b6 PyTorch (>=1.3) Python (3) Ubuntu 16.04 LTS Call for Contribution \u00b6 We are adding more learning-to-rank models all the time. Please submit an issue if there is something you want to have implemented and included. Meanwhile, anyone who are interested in any kinds of contributions and/or collaborations are warmly welcomed. Citation \u00b6 If you use PTRanking in your research, please use the following BibTex entry. @misc{yu2020ptranking, title={PT-Ranking: A Benchmarking Platform for Neural Learning-to-Rank}, author={Hai-Tao Yu}, year={2020}, eprint={2008.13368}, archivePrefix={arXiv}, primaryClass={cs.IR} } Relevant Resources \u00b6 Name Language Deep Learning PTRanking Python PyTorch TF-Ranking Python TensorFlow MatchZoo Python Keras / PyTorch Shoelace Python Chainer LEROT Python x Rank Lib Java x Propensity SVM^Rank C x QuickRank C++ x","title":"Home"},{"location":"#learning-to-rank-in-pytorch","text":"","title":"Learning-to-Rank in PyTorch"},{"location":"#introduction","text":"This open-source project, referred to as PTRanking (Learning-to-Rank in PyTorch) aims to provide scalable and extendable implementations of typical learning-to-rank methods based on PyTorch. On one hand, this project enables a uniform comparison over several benchmark datasets, leading to an in-depth understanding of previous learning-to-rank methods. On the other hand, this project makes it easy to develop and incorporate newly proposed models, so as to expand the territory of techniques on learning-to-rank. Key Features : A number of representative learning-to-rank models for addressing Ad-hoc Ranking and Search Result Diversification , including not only the traditional optimization framework via empirical risk minimization but also the adversarial optimization framework Supports widely used benchmark datasets. Meanwhile, random masking of the ground-truth labels with a specified ratio is also supported Supports different metrics, such as Precision, MAP, nDCG, nERR, alpha-nDCG and ERR-IA. Highly configurable functionalities for fine-tuning hyper-parameters, e.g., grid-search over hyper-parameters of a specific model Provides easy-to-use APIs for developing a new learning-to-rank model","title":"Introduction"},{"location":"#source-code","text":"Please refer to the Github Repository PT-Ranking for detailed implementations.","title":"Source Code"},{"location":"#implemented-models","text":"Typical Learning-to-Rank Methods for Ad-hoc Ranking Model Pointwise RankMSE Pairwise RankNet Listwise ListNet \u30fb ListMLE \u30fb RankCosine \u30fb LambdaRank \u30fb ApproxNDCG \u30fb WassRank \u30fb STListNet \u30fb LambdaLoss Learning-to-Rank Methods for Search Result Diversification Model Score-and-sort strategy MO4SRD \u30fb DALETOR Adversarial Learning-to-Rank Methods for Ad-hoc Ranking Model Pointwise IR_GAN_Point Pairwise IR_GAN_Pair Listwise IR_GAN_List Learning-to-rank Methods Based on Gradient Boosting Decision Trees (GBDT) (based on LightGBM) Model Listwise LightGBMLambdaMART","title":"Implemented models"},{"location":"#references","text":"RankNet : Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd ICML. 89\u201396. RankSVM : Joachims, Thorsten. Optimizing Search Engines Using Clickthrough Data. Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 133\u2013142, 2002. LambdaRank : Christopher J.C. Burges, Robert Ragno, and Quoc Viet Le. 2006. Learning to Rank with Nonsmooth Cost Functions. In Proceedings of NIPS conference. 193\u2013200. ListNet : Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to Rank: From Pairwise Approach to Listwise Approach. In Proceedings of the 24th ICML. 129\u2013136. ListMLE : Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwise Approach to Learning to Rank: Theory and Algorithm. In Proceedings of the 25th ICML. 1192\u20131199. RankCosine : Tao Qin, Xu-Dong Zhang, Ming-Feng Tsai, De-Sheng Wang, Tie-Yan Liu, and Hang Li. 2008. Query-level loss functions for information retrieval. Information Processing and Management 44, 2 (2008), 838\u2013855. AppoxNDCG : Tao Qin, Tie-Yan Liu, and Hang Li. 2010. A general approximation framework for direct optimization of information retrieval measures. Journal of Information Retrieval 13, 4 (2010), 375\u2013397. LambdaMART : Q. Wu, C.J.C. Burges, K. Svore and J. Gao. Adapting Boosting for Information Retrieval Measures. Journal of Information Retrieval, 2007. (We note that the implementation is provided by LightGBM ) IRGAN : Wang, Jun and Yu, Lantao and Zhang, Weinan and Gong, Yu and Xu, Yinghui and Wang, Benyou and Zhang, Peng and Zhang, Dell. IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models. Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, 515\u2013524, 2017. ( Besides the pointwise and pairiwse adversarial learning-to-rank methods introduced in the paper, we also include the listwise version in PT-Ranking ) LambdaLoss Xuanhui Wang, Cheng Li, Nadav Golbandi, Mike Bendersky and Marc Najork. The LambdaLoss Framework for Ranking Metric Optimization. Proceedings of The 27th ACM International Conference on Information and Knowledge Management (CIKM '18), 1313-1322, 2018. WassRank : Hai-Tao Yu, Adam Jatowt, Hideo Joho, Joemon Jose, Xiao Yang and Long Chen. WassRank: Listwise Document Ranking Using Optimal Transport Theory. Proceedings of the 12th International Conference on Web Search and Data Mining (WSDM), 24-32, 2019. Bruch, Sebastian and Han, Shuguang and Bendersky, Michael and Najork, Marc. A Stochastic Treatment of Learning to Rank Scoring Functions. Proceedings of the 13th International Conference on Web Search and Data Mining (WSDM), 61\u201369, 2020. MO4SRD : Hai-Tao Yu. Optimize What You EvaluateWith: Search Result Diversification Based on Metric Optimization. The 36th AAAI Conference on Artificial Intelligence, 2022. DALETOR : Le Yan, Zhen Qin, Rama Kumar Pasumarthi, Xuanhui Wang, Michael Bendersky. Diversification-Aware Learning to Rank using Distributed Representation. In Proceedings of the Web Conference 2021, 127\u2013136.","title":"References"},{"location":"#test-setting","text":"PyTorch (>=1.3) Python (3) Ubuntu 16.04 LTS","title":"Test Setting"},{"location":"#call-for-contribution","text":"We are adding more learning-to-rank models all the time. Please submit an issue if there is something you want to have implemented and included. Meanwhile, anyone who are interested in any kinds of contributions and/or collaborations are warmly welcomed.","title":"Call for Contribution"},{"location":"#citation","text":"If you use PTRanking in your research, please use the following BibTex entry. @misc{yu2020ptranking, title={PT-Ranking: A Benchmarking Platform for Neural Learning-to-Rank}, author={Hai-Tao Yu}, year={2020}, eprint={2008.13368}, archivePrefix={arXiv}, primaryClass={cs.IR} }","title":"Citation"},{"location":"#relevant-resources","text":"Name Language Deep Learning PTRanking Python PyTorch TF-Ranking Python TensorFlow MatchZoo Python Keras / PyTorch Shoelace Python Chainer LEROT Python x Rank Lib Java x Propensity SVM^Rank C x QuickRank C++ x","title":"Relevant Resources"},{"location":"data/data/","text":"Supported Datasets and Formats \u00b6 Popular Benchmark Datasets \u00b6 -- LETOR4.0 (MQ2007 | MQ2008 | MQ2007-semi | MQ2008-semi | MQ2007-list | MQ2008-list ) -- MSLR-WEB (MSLR-WEB10K | MSLR-WEB30K) -- Yahoo! LETOR (including Set1 | Set2) -- Istella (Istella-S | Istella | Istella-X) These above datasets can be directly used once downloaded. Please note that: Semi-supervised datasets (MQ2007-semi | MQ2008-semi) have the same format as that for supervised ranking setting. The only difference is that the semi-supervised datasets in this setting contain both judged and undged query-document pairs ( in training set but not in validation and testing set )(The relevance label \u201c-1\u201d indicates the query-document pair is not judged) while the datasets for supervised ranking contain only judged query-document pair. According to Introducing LETOR 4.0 Datasets , queryLevelNorm version refers to that: conduct query level normalization in the way of using MIN. This data can be directly used for learning. They further provide 5-fold partitions of this version for cross fold validation. Thus there is no need to perform query_level_scale again for {MQ2007_super | MQ2008_super | MQ2007_semi | MQ2008_semi}. But for {MSLRWEB10K | MSLRWEB30K}, the query-level normalization is not conducted yet . For Yahoo! LETOR, the query-level normalization is already done. For Istella! LETOR, the query-level normalization is not conducted yet . We note that ISTELLA contains extremely large features, e.g., 1.79769313486e+308, we replace features of this kind with a constant 1000000. LibSVM formats \u00b6 PT-Ranking currently supports to ingest data with the LibSVM formats LETOR datasets in LibSVM format \\<ground-truth label int> qid\\: : ... : For example: 4 qid:105 2:0.4 8:0.7 50:0.5 1 qid:105 5:0.5 30:0.7 32:0.4 48:0.53 0 qid:210 4:0.9 38:0.01 39:0.5 45:0.7 1 qid:210 1:0.2 8:0.9 31:0.93 40:0.6 The above sample dataset includes two queries, the query \"105\" has 2 documents, the corresponding ground-truth labels are 4 and 1, respectively. Converting LETOR datasets into LibSVM format with a corresponding group file This functionality is required when using the implementation of LambdaMART provided in LightGBM .","title":"Data"},{"location":"data/data/#supported-datasets-and-formats","text":"","title":"Supported Datasets and Formats"},{"location":"data/data/#popular-benchmark-datasets","text":"-- LETOR4.0 (MQ2007 | MQ2008 | MQ2007-semi | MQ2008-semi | MQ2007-list | MQ2008-list ) -- MSLR-WEB (MSLR-WEB10K | MSLR-WEB30K) -- Yahoo! LETOR (including Set1 | Set2) -- Istella (Istella-S | Istella | Istella-X) These above datasets can be directly used once downloaded. Please note that: Semi-supervised datasets (MQ2007-semi | MQ2008-semi) have the same format as that for supervised ranking setting. The only difference is that the semi-supervised datasets in this setting contain both judged and undged query-document pairs ( in training set but not in validation and testing set )(The relevance label \u201c-1\u201d indicates the query-document pair is not judged) while the datasets for supervised ranking contain only judged query-document pair. According to Introducing LETOR 4.0 Datasets , queryLevelNorm version refers to that: conduct query level normalization in the way of using MIN. This data can be directly used for learning. They further provide 5-fold partitions of this version for cross fold validation. Thus there is no need to perform query_level_scale again for {MQ2007_super | MQ2008_super | MQ2007_semi | MQ2008_semi}. But for {MSLRWEB10K | MSLRWEB30K}, the query-level normalization is not conducted yet . For Yahoo! LETOR, the query-level normalization is already done. For Istella! LETOR, the query-level normalization is not conducted yet . We note that ISTELLA contains extremely large features, e.g., 1.79769313486e+308, we replace features of this kind with a constant 1000000.","title":"Popular Benchmark Datasets"},{"location":"data/data/#libsvm-formats","text":"PT-Ranking currently supports to ingest data with the LibSVM formats LETOR datasets in LibSVM format \\<ground-truth label int> qid\\: : ... : For example: 4 qid:105 2:0.4 8:0.7 50:0.5 1 qid:105 5:0.5 30:0.7 32:0.4 48:0.53 0 qid:210 4:0.9 38:0.01 39:0.5 45:0.7 1 qid:210 1:0.2 8:0.9 31:0.93 40:0.6 The above sample dataset includes two queries, the query \"105\" has 2 documents, the corresponding ground-truth labels are 4 and 1, respectively. Converting LETOR datasets into LibSVM format with a corresponding group file This functionality is required when using the implementation of LambdaMART provided in LightGBM .","title":"LibSVM formats"},{"location":"how_to_start/Configuration/","text":"Configuration Strategy \u00b6 An easy-to-use configuration is necessary for any ML library. PT-Ranking offers a self-contained strategy. In other words, we appeal to particularly designed class objects for setting. For example, DataSetting for data loading, EvalSetting for evaluation setting and ModelParameter for a model's parameter setting. Moreover, configuration with json files is also supported for DataSetting, EvalSetting and ModelParameter, which is the recommended way. Thanks to this strategy, on one hand, we can initialize the settings for data-loading, evaluation, and models in a simple way. In particular, the parameter setting of a model is self-contained, and easy to customize. On the other hand, given the specified setting, e.g., settings with json files, it is very easy to reproduce an experiment. Setting on Loading A Dataset \u00b6 When loading a dataset, the meta-information and preprocessing are specified with DataSetting . Taking the json file for initializing DataSetting for example, \"data_id\":\"MQ2008_Super\", # the ID of an adopted dataset \"dir_data\":\"/Users/dryuhaitao/WorkBench/Corpus/LETOR4.0/MQ2008/\", # the location of an adopted dataset \"min_docs\":[10], # the minimum number of documents per query. Otherwise, the query instance is not used. \"min_rele\":[1], # the minimum number of relevant documents. Otherwise, the query instance is not used. \"sample_rankings_per_q\":[1], # the sample rankings per query \"binary_rele\":[false], # whether convert multi-graded labels into binary labels \"unknown_as_zero\":[false], # whether convert '-1' (i.e., unlabeled documents) as zero \"presort\":[true] # whether sort documents based on ground-truth labels in advance Setting on Evaluation \u00b6 When testing a specific learning-to-rank method, the evaluation details are specified with EvalSetting . Taking the json file for initializing EvalSetting for example, \"dir_output\":\"/Users/dryuhaitao/WorkBench/CodeBench/Bench_Output/NeuralLTR/Listwise/\", # output directory of results \"epochs\":100, # the number of epoches for training \"do_validation\":true, # perform validation or not \"vali_k\":5, # the cutoff value for validation, e.g., nDCG@5 \"cutoffs\":[1, 3, 5, 10, 20, 50], # the cutoff values for evaluation \"loss_guided\":false, # whether the selection of trained model is based on loss function or validation \"do_log\":true, # logging the training outputs \"log_step\":2, # the step-size of logging \"do_summary\":false, \"mask_label\":false, # mask ground-truth labels \"mask_type\":[\"rand_mask_all\"], \"mask_ratio\":[0.2] Parameter Setting \u00b6 Parameters for Base Scoring Function \u00b6 For most learning-to-rank methods, PT-Ranking offers deep neural networks as the basis to construct a scoring function. Therefore, we use ScoringFunctionParameter to specify the details, such as the number of layers and activation function. Taking the json file for initializing ScoringFunctionParameter for example, \"BN\":[true], # batch normalization \"RD\":[false], # residual module \"layers\":[5], # number of layers \"apply_tl_af\":[true], # use activation function for the last layer \"hd_hn_tl_af\":[\"R\"] # type of activation function Parameters for Loss Function \u00b6 Besides the configuration of the scoring function, for some relatively complex learning-to-rank methods, we also need to specify some parameters for the loss function. In this case, it is required to develop the subclass ModelAParameter by inheriting ModelParameter and customizing the functions, such as to_para_string(), default_para_dict() and grid_search(). Please refer to LambdaRankParameter as an example. Prepare Configuration Json Files \u00b6 When evaluating a method, two json files are commonly required: Data_Eval_ScoringFunction.json , which specifies the detailed settings for data loading (i.e, DataSetting), evaluation (i.e, EvalSetting) and the parameters for base scoring function (i.e, ScoringFunctionParameter). Please refer to Data_Eval_ScoringFunction as an example. XParameter , which specifies the parameters for model X . Please refer to LambdaRankParameter as an example.","title":"Configuration"},{"location":"how_to_start/Configuration/#configuration-strategy","text":"An easy-to-use configuration is necessary for any ML library. PT-Ranking offers a self-contained strategy. In other words, we appeal to particularly designed class objects for setting. For example, DataSetting for data loading, EvalSetting for evaluation setting and ModelParameter for a model's parameter setting. Moreover, configuration with json files is also supported for DataSetting, EvalSetting and ModelParameter, which is the recommended way. Thanks to this strategy, on one hand, we can initialize the settings for data-loading, evaluation, and models in a simple way. In particular, the parameter setting of a model is self-contained, and easy to customize. On the other hand, given the specified setting, e.g., settings with json files, it is very easy to reproduce an experiment.","title":"Configuration Strategy"},{"location":"how_to_start/Configuration/#setting-on-loading-a-dataset","text":"When loading a dataset, the meta-information and preprocessing are specified with DataSetting . Taking the json file for initializing DataSetting for example, \"data_id\":\"MQ2008_Super\", # the ID of an adopted dataset \"dir_data\":\"/Users/dryuhaitao/WorkBench/Corpus/LETOR4.0/MQ2008/\", # the location of an adopted dataset \"min_docs\":[10], # the minimum number of documents per query. Otherwise, the query instance is not used. \"min_rele\":[1], # the minimum number of relevant documents. Otherwise, the query instance is not used. \"sample_rankings_per_q\":[1], # the sample rankings per query \"binary_rele\":[false], # whether convert multi-graded labels into binary labels \"unknown_as_zero\":[false], # whether convert '-1' (i.e., unlabeled documents) as zero \"presort\":[true] # whether sort documents based on ground-truth labels in advance","title":"Setting on Loading A Dataset"},{"location":"how_to_start/Configuration/#setting-on-evaluation","text":"When testing a specific learning-to-rank method, the evaluation details are specified with EvalSetting . Taking the json file for initializing EvalSetting for example, \"dir_output\":\"/Users/dryuhaitao/WorkBench/CodeBench/Bench_Output/NeuralLTR/Listwise/\", # output directory of results \"epochs\":100, # the number of epoches for training \"do_validation\":true, # perform validation or not \"vali_k\":5, # the cutoff value for validation, e.g., nDCG@5 \"cutoffs\":[1, 3, 5, 10, 20, 50], # the cutoff values for evaluation \"loss_guided\":false, # whether the selection of trained model is based on loss function or validation \"do_log\":true, # logging the training outputs \"log_step\":2, # the step-size of logging \"do_summary\":false, \"mask_label\":false, # mask ground-truth labels \"mask_type\":[\"rand_mask_all\"], \"mask_ratio\":[0.2]","title":"Setting on Evaluation"},{"location":"how_to_start/Configuration/#parameter-setting","text":"","title":"Parameter Setting"},{"location":"how_to_start/Configuration/#parameters-for-base-scoring-function","text":"For most learning-to-rank methods, PT-Ranking offers deep neural networks as the basis to construct a scoring function. Therefore, we use ScoringFunctionParameter to specify the details, such as the number of layers and activation function. Taking the json file for initializing ScoringFunctionParameter for example, \"BN\":[true], # batch normalization \"RD\":[false], # residual module \"layers\":[5], # number of layers \"apply_tl_af\":[true], # use activation function for the last layer \"hd_hn_tl_af\":[\"R\"] # type of activation function","title":"Parameters for Base Scoring Function"},{"location":"how_to_start/Configuration/#parameters-for-loss-function","text":"Besides the configuration of the scoring function, for some relatively complex learning-to-rank methods, we also need to specify some parameters for the loss function. In this case, it is required to develop the subclass ModelAParameter by inheriting ModelParameter and customizing the functions, such as to_para_string(), default_para_dict() and grid_search(). Please refer to LambdaRankParameter as an example.","title":"Parameters for Loss Function"},{"location":"how_to_start/Configuration/#prepare-configuration-json-files","text":"When evaluating a method, two json files are commonly required: Data_Eval_ScoringFunction.json , which specifies the detailed settings for data loading (i.e, DataSetting), evaluation (i.e, EvalSetting) and the parameters for base scoring function (i.e, ScoringFunctionParameter). Please refer to Data_Eval_ScoringFunction as an example. XParameter , which specifies the parameters for model X . Please refer to LambdaRankParameter as an example.","title":"Prepare Configuration Json Files"},{"location":"how_to_start/Develop_A_New_Model/","text":"Develop Your Own Learning-to-Rank Method \u00b6 PT-Ranking offers deep neural networks as the basis to construct a scoring function based on PyTorch and can thus fully leverage the advantages of PyTorch. NeuralRanker is a class that represents a general learning-to-rank model. A key component of NeuralRanker is the neural scoring function. The configurable hyper-parameters include activation function, number of layers, number of neurons per layer, etc. All specific learning-to-rank models inherit NeuralRanker and mainly differ in the way of computing the training loss. The following figure shows the main step in developing a new learning-to-rank model based on Empirical Risk Minimization, where batch_preds and batch_stds correspond to outputs of the scoring function and ground-truth lables, respectively. We can observe that the main work is to define the surrogate loss function. When incorporating a newly developed model (say ModelA), it is commonly required to develop the subclass ModelAParameter by inheriting ModelParameter and customizing the functions, such as to_para_string(), default_para_dict() and grid_search(). Please refer to Configuration for detailed description on parameter setting and LambdaRankParameter as an example. To fully leverage PT-Ranking, one needs to be familiar with PyTorch . For detailed introduction on learning-to-rank, please refer to the book: Learning to Rank for Information Retrieval .","title":"Develop A New Model"},{"location":"how_to_start/Develop_A_New_Model/#develop-your-own-learning-to-rank-method","text":"PT-Ranking offers deep neural networks as the basis to construct a scoring function based on PyTorch and can thus fully leverage the advantages of PyTorch. NeuralRanker is a class that represents a general learning-to-rank model. A key component of NeuralRanker is the neural scoring function. The configurable hyper-parameters include activation function, number of layers, number of neurons per layer, etc. All specific learning-to-rank models inherit NeuralRanker and mainly differ in the way of computing the training loss. The following figure shows the main step in developing a new learning-to-rank model based on Empirical Risk Minimization, where batch_preds and batch_stds correspond to outputs of the scoring function and ground-truth lables, respectively. We can observe that the main work is to define the surrogate loss function. When incorporating a newly developed model (say ModelA), it is commonly required to develop the subclass ModelAParameter by inheriting ModelParameter and customizing the functions, such as to_para_string(), default_para_dict() and grid_search(). Please refer to Configuration for detailed description on parameter setting and LambdaRankParameter as an example. To fully leverage PT-Ranking, one needs to be familiar with PyTorch . For detailed introduction on learning-to-rank, please refer to the book: Learning to Rank for Information Retrieval .","title":"Develop Your Own Learning-to-Rank Method"},{"location":"how_to_start/Get_Started/","text":"Requirements \u00b6 Prepare a virtual environment with Python 3.* via conda , venv or others. Install Pytorch following the instructions Install scikit-learn following the instructions Command-line Usage \u00b6 Download source code git clone https://github.com/wildltr/ptranking Download Supported Datasets , such as MQ2008 and unrar the .rar file. wget \"https://lyoz5a.ch.files.1drv.com/y4mM8g8v4d2mFfO5djKT-ELADpDDRcsVwXRSaZu-9rlOlgvW62Qeuc8hFe_wr6m5NZMnUSEfr6QpMP81ZIQIiwI4BnoHmIZT9Sraf53AmhhIfLi531DOKYZTy4MtDHbBC7dn_Z9DSKvLJZhERPIamAXCrONg7WrFPiG0sTpOXl3-YEYZ1scTslmNyg2a__3YalWRMyEIipY56sy97pb68Sdww\" -O MQ2008.rar Prepare the required json files (Data_Eval_ScoringFunction.json and XParameter.json, please refer to Configuration for more information) for specifying evaluation details. Run the following command script on your Terminal/Command Prompt: (1) Without using GPU python pt_ranking.py -model ListMLE -dir_json /home/dl-box/WorkBench/Dropbox/CodeBench/GitPool/wildltr_ptranking/testing/ltr_adhoc/json/ (2) Using GPU python pt_ranking.py -cuda 0 -model ListMLE -dir_json /home/dl-box/WorkBench/Dropbox/CodeBench/GitPool/wildltr_ptranking/testing/ltr_adhoc/json/ The meaning of each supported argument is: optional arguments: -h, --help show this help message and exit -cuda CUDA specify the gpu id if needed, such as 0 or 1. -model MODEL specify the learning-to-rank method -debug quickly check the setting in a debug mode -dir_json DIR_JSON the path of json files specifying the evaluation details. PyCharm Usage \u00b6 Install PyCharm (either Professional version or Community version) Download source code git clone https://github.com/wildltr/ptranking Open the unzipped source code with PyCharm as a new project Test the supported learning-to-rank models by selectively running the following files, where the setting arguments can be changed accordingly testing/ltr_adhoc/testing_ltr_adhoc.py testing/ltr_adversarial/testing_ltr_adversarial.py testing/ltr_tree/testing_ltr_tree.py Python-package Usage \u00b6 TBA Install ptranking: pip install ptranking Demo Scripts \u00b6 To get a taste of learning-to-rank models without writing any code, you could try the following script. You just need to specify the model name, the dataset id, as well as the directories for input and output. Jupyter Notebook example on RankNet & LambdaRank To get familiar with the process of data loading, you could try the following script, namely, get the statistics of a dataset. Jupyter Notebook example on getting dataset statistics","title":"Get Started"},{"location":"how_to_start/Get_Started/#requirements","text":"Prepare a virtual environment with Python 3.* via conda , venv or others. Install Pytorch following the instructions Install scikit-learn following the instructions","title":"Requirements"},{"location":"how_to_start/Get_Started/#command-line-usage","text":"Download source code git clone https://github.com/wildltr/ptranking Download Supported Datasets , such as MQ2008 and unrar the .rar file. wget \"https://lyoz5a.ch.files.1drv.com/y4mM8g8v4d2mFfO5djKT-ELADpDDRcsVwXRSaZu-9rlOlgvW62Qeuc8hFe_wr6m5NZMnUSEfr6QpMP81ZIQIiwI4BnoHmIZT9Sraf53AmhhIfLi531DOKYZTy4MtDHbBC7dn_Z9DSKvLJZhERPIamAXCrONg7WrFPiG0sTpOXl3-YEYZ1scTslmNyg2a__3YalWRMyEIipY56sy97pb68Sdww\" -O MQ2008.rar Prepare the required json files (Data_Eval_ScoringFunction.json and XParameter.json, please refer to Configuration for more information) for specifying evaluation details. Run the following command script on your Terminal/Command Prompt: (1) Without using GPU python pt_ranking.py -model ListMLE -dir_json /home/dl-box/WorkBench/Dropbox/CodeBench/GitPool/wildltr_ptranking/testing/ltr_adhoc/json/ (2) Using GPU python pt_ranking.py -cuda 0 -model ListMLE -dir_json /home/dl-box/WorkBench/Dropbox/CodeBench/GitPool/wildltr_ptranking/testing/ltr_adhoc/json/ The meaning of each supported argument is: optional arguments: -h, --help show this help message and exit -cuda CUDA specify the gpu id if needed, such as 0 or 1. -model MODEL specify the learning-to-rank method -debug quickly check the setting in a debug mode -dir_json DIR_JSON the path of json files specifying the evaluation details.","title":"Command-line Usage"},{"location":"how_to_start/Get_Started/#pycharm-usage","text":"Install PyCharm (either Professional version or Community version) Download source code git clone https://github.com/wildltr/ptranking Open the unzipped source code with PyCharm as a new project Test the supported learning-to-rank models by selectively running the following files, where the setting arguments can be changed accordingly testing/ltr_adhoc/testing_ltr_adhoc.py testing/ltr_adversarial/testing_ltr_adversarial.py testing/ltr_tree/testing_ltr_tree.py","title":"PyCharm Usage"},{"location":"how_to_start/Get_Started/#python-package-usage","text":"TBA Install ptranking: pip install ptranking","title":"Python-package Usage"},{"location":"how_to_start/Get_Started/#demo-scripts","text":"To get a taste of learning-to-rank models without writing any code, you could try the following script. You just need to specify the model name, the dataset id, as well as the directories for input and output. Jupyter Notebook example on RankNet & LambdaRank To get familiar with the process of data loading, you could try the following script, namely, get the statistics of a dataset. Jupyter Notebook example on getting dataset statistics","title":"Demo Scripts"},{"location":"ltr_adhoc/Lambda_Framework/","text":"From RankNet to LambdaRank to LambdaMART: A Revisit \u00b6 Please refer to ptranking_lambda_framework.ipynb for detailed description.","title":"Lambda Framework"},{"location":"ltr_adhoc/Lambda_Framework/#from-ranknet-to-lambdarank-to-lambdamart-a-revisit","text":"Please refer to ptranking_lambda_framework.ipynb for detailed description.","title":"From RankNet to LambdaRank to LambdaMART: A Revisit"},{"location":"ltr_adhoc/about/","text":"About LTR_Adhoc \u00b6 By LTR_Adhoc , we refer to the traditional learning-to-rank methods based on the Empirical Risk Minimization Framework, which is detailed in ptranking_empirical_risk_minimization.ipynb . Major learning-to-rank approaches can be classified into three categories: pointwise , pairwise , and listwise . The key distinctions are the underlying hypotheses, loss functions, the input and output spaces. The typical pointwise approaches include regression-based [1], classification-based [2], and ordinal regression-based algorithms [3, 4]. The loss functions of these algorithms is defined on the basis of each individual document. The pairwise approaches care about the relative order between two documents. The goal of learning is to maximize the number of correctly ordered document pairs. The assumption is that the optimal ranking of documents can be achieved if all the document pairs are correctly ordered. Towards this end, many representative methods have been proposed [5,6,7,8,9]. The listwise approaches take all the documents associated with the same query in the training data as the input. In particular, there are two types of loss functions when performing listwise learning. For the first type, the loss function is related to a specific evaluation metric (e.g., nDCG and ERR). Due to the non-differentiability and non-decomposability of the commonly used metrics, the methods of this type either try to optimize the upper bounds as surrogate objective functions [10, 11, 12] or approximate the target metric using some smooth functions [13, 14, 15]. However, there are still some open issues regarding the first type methods. On one hand, some adopted surrogate functions or approximated metrics are not convex, which makes it hard to optimize. On the other hand, the relationship between the surrogate function and the adopted metric has not been sufficiently investigated, which makes it unclear whether optimizing the surrogate functions can indeed optimize the target metric. For the second type, the loss function is not explicitly related to a specific evaluation metric. The loss function reflects the discrepancy between the predicted ranking and the ground-truth ranking. Example algorithms include []. Although no particular evaluation metrics are directly involved and optimized here, it is possible that the learned ranking function can achieve good performance in terms of evaluation metrics. References \u00b6 [1] David Cossock and Tong Zhang. 2006. Subset Ranking Using Regression. In Proceedings of the 19th Annual Conference on Learning Theory. 605\u2013619. [2] Ramesh Nallapati. 2004. Discriminative Models for Information Retrieval. In Proceedings of the 27th SIGIR. 64\u201371. [3] Wei Chu and Zoubin Ghahramani. 2005. Gaussian Processes for Ordinal Regression. Journal of Machine Learning Research 6 (2005), 1019\u20131041. [4] Wei Chu and S. Sathiya Keerthi. 2005. New Approaches to Support Vector Ordinal Regression. In Proceedings of the 22nd ICML. 145\u2013152. [5] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd ICML. 89\u201396. [6] Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram Singer. 2003. An Efficient Boosting Algorithm for Combining Preferences. Journal of Machine Learning Research 4 (2003), 933\u2013969. [7] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the 8th KDD. 133\u2013142. [8] Libin Shen and Aravind K. Joshi. 2005. Ranking and Reranking with Perceptron. Machine Learning 60, 1-3 (2005), 73\u201396. [9] Fajie Yuan, Guibing Guo, Joemon Jose, Long Chen, Hai-Tao Yu, andWeinan Zhang. LambdaFM: Learning Optimal Ranking with Factorization Machines Using Lambda Surrogates. In Proceedings of the 25th CIKM. 227\u2013236. [10] Olivier Chapelle, Quoc Le, and Alex Smola. 2007. Large margin optimization of ranking measures. In NIPS workshop on Machine Learning for Web Search. [11] Jun Xu and Hang Li. 2007. AdaRank: a boosting algorithm for information retrieval. In Proceedings of the 30th SIGIR. 391\u2013398. [12] Yisong Yue, Thomas Finley, Filip Radlinski, and Thorsten Joachims. 2007. A Support Vector Method for Optimizing Average Precision. In Proceedings of the 30th SIGIR. 271\u2013278. [13] John Guiver and Edward Snelson. 2008. Learning to Rank with SoftRank and Gaussian Processes. In Proceedings of the 31st SIGIR. 259\u2013266. [14] Tao Qin, Tie-Yan Liu, and Hang Li. 2010. A general approximation framework for direct optimization of information retrieval measures. Journal of Information Retrieval 13, 4 (2010), 375\u2013397. [15] Michael Taylor, John Guiver, Stephen Robertson, and Tom Minka. 2008. SoftRank: Optimizing Non-smooth Rank Metrics. In Proceedings of the 1st WSDM. 77\u201386. [16] Christopher J.C. Burges, Robert Ragno, and Quoc Viet Le. 2006. Learning to Rank with Nonsmooth Cost Functions. In Proceedings of NIPS conference. 193\u2013200. [17] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to Rank: From Pairwise Approach to Listwise Approach. In Proceedings of the 24th ICML. 129\u2013136. [18] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwise Approach to Learning to Rank: Theory and Algorithm. In Proceedings of the 25th ICML. 1192\u20131199.","title":"About"},{"location":"ltr_adhoc/about/#about-ltr_adhoc","text":"By LTR_Adhoc , we refer to the traditional learning-to-rank methods based on the Empirical Risk Minimization Framework, which is detailed in ptranking_empirical_risk_minimization.ipynb . Major learning-to-rank approaches can be classified into three categories: pointwise , pairwise , and listwise . The key distinctions are the underlying hypotheses, loss functions, the input and output spaces. The typical pointwise approaches include regression-based [1], classification-based [2], and ordinal regression-based algorithms [3, 4]. The loss functions of these algorithms is defined on the basis of each individual document. The pairwise approaches care about the relative order between two documents. The goal of learning is to maximize the number of correctly ordered document pairs. The assumption is that the optimal ranking of documents can be achieved if all the document pairs are correctly ordered. Towards this end, many representative methods have been proposed [5,6,7,8,9]. The listwise approaches take all the documents associated with the same query in the training data as the input. In particular, there are two types of loss functions when performing listwise learning. For the first type, the loss function is related to a specific evaluation metric (e.g., nDCG and ERR). Due to the non-differentiability and non-decomposability of the commonly used metrics, the methods of this type either try to optimize the upper bounds as surrogate objective functions [10, 11, 12] or approximate the target metric using some smooth functions [13, 14, 15]. However, there are still some open issues regarding the first type methods. On one hand, some adopted surrogate functions or approximated metrics are not convex, which makes it hard to optimize. On the other hand, the relationship between the surrogate function and the adopted metric has not been sufficiently investigated, which makes it unclear whether optimizing the surrogate functions can indeed optimize the target metric. For the second type, the loss function is not explicitly related to a specific evaluation metric. The loss function reflects the discrepancy between the predicted ranking and the ground-truth ranking. Example algorithms include []. Although no particular evaluation metrics are directly involved and optimized here, it is possible that the learned ranking function can achieve good performance in terms of evaluation metrics.","title":"About LTR_Adhoc"},{"location":"ltr_adhoc/about/#references","text":"[1] David Cossock and Tong Zhang. 2006. Subset Ranking Using Regression. In Proceedings of the 19th Annual Conference on Learning Theory. 605\u2013619. [2] Ramesh Nallapati. 2004. Discriminative Models for Information Retrieval. In Proceedings of the 27th SIGIR. 64\u201371. [3] Wei Chu and Zoubin Ghahramani. 2005. Gaussian Processes for Ordinal Regression. Journal of Machine Learning Research 6 (2005), 1019\u20131041. [4] Wei Chu and S. Sathiya Keerthi. 2005. New Approaches to Support Vector Ordinal Regression. In Proceedings of the 22nd ICML. 145\u2013152. [5] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd ICML. 89\u201396. [6] Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram Singer. 2003. An Efficient Boosting Algorithm for Combining Preferences. Journal of Machine Learning Research 4 (2003), 933\u2013969. [7] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the 8th KDD. 133\u2013142. [8] Libin Shen and Aravind K. Joshi. 2005. Ranking and Reranking with Perceptron. Machine Learning 60, 1-3 (2005), 73\u201396. [9] Fajie Yuan, Guibing Guo, Joemon Jose, Long Chen, Hai-Tao Yu, andWeinan Zhang. LambdaFM: Learning Optimal Ranking with Factorization Machines Using Lambda Surrogates. In Proceedings of the 25th CIKM. 227\u2013236. [10] Olivier Chapelle, Quoc Le, and Alex Smola. 2007. Large margin optimization of ranking measures. In NIPS workshop on Machine Learning for Web Search. [11] Jun Xu and Hang Li. 2007. AdaRank: a boosting algorithm for information retrieval. In Proceedings of the 30th SIGIR. 391\u2013398. [12] Yisong Yue, Thomas Finley, Filip Radlinski, and Thorsten Joachims. 2007. A Support Vector Method for Optimizing Average Precision. In Proceedings of the 30th SIGIR. 271\u2013278. [13] John Guiver and Edward Snelson. 2008. Learning to Rank with SoftRank and Gaussian Processes. In Proceedings of the 31st SIGIR. 259\u2013266. [14] Tao Qin, Tie-Yan Liu, and Hang Li. 2010. A general approximation framework for direct optimization of information retrieval measures. Journal of Information Retrieval 13, 4 (2010), 375\u2013397. [15] Michael Taylor, John Guiver, Stephen Robertson, and Tom Minka. 2008. SoftRank: Optimizing Non-smooth Rank Metrics. In Proceedings of the 1st WSDM. 77\u201386. [16] Christopher J.C. Burges, Robert Ragno, and Quoc Viet Le. 2006. Learning to Rank with Nonsmooth Cost Functions. In Proceedings of NIPS conference. 193\u2013200. [17] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to Rank: From Pairwise Approach to Listwise Approach. In Proceedings of the 24th ICML. 129\u2013136. [18] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwise Approach to Learning to Rank: Theory and Algorithm. In Proceedings of the 25th ICML. 1192\u20131199.","title":"References"},{"location":"ltr_adversarial/IRGAN/","text":"IRGAN \u00b6 Please refer to the following paper for detailed description. IRGAN : Wang, Jun and Yu, Lantao and Zhang, Weinan and Gong, Yu and Xu, Yinghui and Wang, Benyou and Zhang, Peng and Zhang, Dell. IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models. Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, 515\u2013524, 2017.","title":"IRGAN"},{"location":"ltr_adversarial/IRGAN/#irgan","text":"Please refer to the following paper for detailed description. IRGAN : Wang, Jun and Yu, Lantao and Zhang, Weinan and Gong, Yu and Xu, Yinghui and Wang, Benyou and Zhang, Peng and Zhang, Dell. IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models. Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, 515\u2013524, 2017.","title":"IRGAN"},{"location":"ltr_adversarial/about/","text":"About LTR_Adversarial \u00b6 By LTR_Adversarial , we refer to the adversarial learning-to-rank methods inspired by the generative adversarial network (GAN) and its variants.","title":"About"},{"location":"ltr_adversarial/about/#about-ltr_adversarial","text":"By LTR_Adversarial , we refer to the adversarial learning-to-rank methods inspired by the generative adversarial network (GAN) and its variants.","title":"About LTR_Adversarial"},{"location":"ltr_diversification/Direct_Metric_Optimization/","text":"Please refer to the following papers for detailed description. Hai-Tao Yu. Optimize What You EvaluateWith: Search Result Diversification Based on Metric Optimization. The 36th AAAI Conference on Artificial Intelligence, 2022. Le Yan, Zhen Qin, Rama Kumar Pasumarthi, Xuanhui Wang, Michael Bendersky. Diversification-Aware Learning to Rank using Distributed Representation. In Proceedings of the Web Conference 2021, 127\u2013136.","title":"Direct Metric Optimization"},{"location":"ltr_diversification/about/","text":"About LTR_Diversification \u00b6 By LTR_Diversification , we refer to the learning-to-rank methods for search result diversification.","title":"About"},{"location":"ltr_diversification/about/#about-ltr_diversification","text":"By LTR_Diversification , we refer to the learning-to-rank methods for search result diversification.","title":"About LTR_Diversification"},{"location":"ltr_gbm/LambdaMART/","text":"From RankNet to LambdaRank to LambdaMART: A Revisit \u00b6 Please refer to ptranking_gbm.ipynb for detailed description.","title":"LambdaMART"},{"location":"ltr_gbm/LambdaMART/#from-ranknet-to-lambdarank-to-lambdamart-a-revisit","text":"Please refer to ptranking_gbm.ipynb for detailed description.","title":"From RankNet to LambdaRank to LambdaMART: A Revisit"},{"location":"ltr_gbm/about/","text":"About LTR_GBM \u00b6 By LTR_GBM , we refer to the learning-to-rank methods based on the framework of Gradient Boosting Machine (GBM).","title":"About"},{"location":"ltr_gbm/about/#about-ltr_gbm","text":"By LTR_GBM , we refer to the learning-to-rank methods based on the framework of Gradient Boosting Machine (GBM).","title":"About LTR_GBM"},{"location":"metric/metric/","text":"In PT-Ranking, the widely used IR metrics, such as precision , average precision , nDCG and ERR , are included. Definition \u00b6 For the definition of each metric, please refer to ptranking_ir_metric.ipynb for a brief description. Implementation \u00b6 For the implementation based on PyTorch, please refer to adhoc_metric.py and testing_metric.py for a reference.","title":"Metric"},{"location":"metric/metric/#definition","text":"For the definition of each metric, please refer to ptranking_ir_metric.ipynb for a brief description.","title":"Definition"},{"location":"metric/metric/#implementation","text":"For the implementation based on PyTorch, please refer to adhoc_metric.py and testing_metric.py for a reference.","title":"Implementation"}]}